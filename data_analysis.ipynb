{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harmonization Parameters Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sensor_data(file_prefix, num_sensors=15,api=False):\n",
    "    \n",
    "    if api:\n",
    "        # file_prefix = 'tsi_v3_api/telemetry_outputs/Raw_Edited.csv'\n",
    "        df = pd.read_csv(file_prefix)\n",
    "        # Group by 'model' and create a dictionary of DataFrames\n",
    "        model_dfs = {}\n",
    "        sensor_dataframes = []\n",
    "        for model, group in df.groupby('friendly_name'):\n",
    "            model_dfs[model] = group\n",
    "\n",
    "        rename_dict = {\n",
    "                'PM1.0 (ug/m3)': 'PM 1.0',\n",
    "                'PM2.5 (ug/m3)': 'PM 2.5',\n",
    "                'PM4.0 (ug/m3)': 'PM 4.0',\n",
    "                'PM10 (ug/m3)': 'PM 10',\n",
    "                'PM0.5 NC (#/cm3)': 'NC 0.5',\n",
    "                'PM1.0 NC (#/cm3)': 'NC 1.0',\n",
    "                'PM2.5 NC (#/cm3)': 'NC 2.5',\n",
    "                'PM4.0 NC (#/cm3)': 'NC 4.0',\n",
    "                'PM10 NC (#/cm3)': 'NC 10',\n",
    "                'Typical Particle Size (um)': 'Typical Particle Size',\n",
    "                'Temperature (Celsius)': 'Temperature',\n",
    "                'Relative Humidity (%)': 'Relative Humidity',\n",
    "                'timestamp' : 'Timestamp'\n",
    "        }\n",
    "        # Access individual DataFrames\n",
    "        for keys in model_dfs.keys():\n",
    "            # print(f\"DataFrame for Model {model}:\")\n",
    "            # print(model_df)\n",
    "                \n",
    "            model_dfs[keys] = model_dfs[keys].rename(columns=rename_dict)\n",
    "            # print(\"\\n\")\n",
    "            sensor_dataframes.append(model_dfs[keys])\n",
    "        return sensor_dataframes\n",
    "        \n",
    "    else:\n",
    "        sensor_dataframes = []\n",
    "        for i in range(1, num_sensors + 1):\n",
    "            filename = f'{file_prefix}{i:02d}.csv'\n",
    "            sensor_df = pd.read_csv(filename)\n",
    "            sensor_dataframes.append(sensor_df)\n",
    "        return sensor_dataframes\n",
    "\n",
    "# sensor_dataframes = read_sensor_data('data/Jun1-Jun7/Indoor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Data from Darrens Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/75048986/way-to-temporarily-change-the-directory-in-python-to-execute-code-without-affect\n",
    "\n",
    "import contextlib\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def new_cd(x):\n",
    "    d = os.getcwd()\n",
    "\n",
    "    # This could raise an exception, but it's probably\n",
    "    # best to let it propagate and let the caller\n",
    "    # deal with it, since they requested x\n",
    "    os.chdir(x)\n",
    "\n",
    "    try:\n",
    "        yield\n",
    "\n",
    "    finally:\n",
    "        # This could also raise an exception, but you *really*\n",
    "        # aren't equipped to figure out what went wrong if the\n",
    "        # old working directory can't be restored.\n",
    "        os.chdir(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsi_v3_api.get_all_raw_data import device_list, client_token, main, get_and_flatten_telemetry\n",
    "\n",
    "work_dir = r'tsi_v3_api/'\n",
    "\n",
    "# with os.chdir(work_dir):\n",
    "with new_cd(work_dir):\n",
    "\n",
    "    from tsi_v3_api.get_all_raw_data import device_list, client_token, main, get_and_flatten_telemetry\n",
    "    secrets_PATH = r'account_auth_info/secrets.csv'\n",
    "    main(secrets_PATH, days_duration=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add relavant details to secrets\n",
    "\n",
    "# work_dir = r'tsi_v3_api/'\n",
    "# os.chdir(work_dir)\n",
    "# secrets_PATH = r'account_auth_info/secrets.csv'\n",
    "# main(secrets_PATH,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_prefix = 'tsi_v3_api/telemetry_outputs/Raw_Edited.csv'\n",
    "sensor_dataframes = read_sensor_data(file_prefix,api=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensor_dataframes[0].to_csv('tsi_v3_api/telemetry_outputs/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sensor_data(file_prefix, num_sensors=15):\n",
    "    sensor_dataframes = []\n",
    "    for i in range(1, num_sensors + 1):\n",
    "        filename = f'data/May25-May31/{file_prefix}{i:02d}.csv'\n",
    "        sensor_df = pd.read_csv(filename)\n",
    "        sensor_dataframes.append(sensor_df)\n",
    "    return sensor_dataframes\n",
    "\n",
    "sensor_dataframes = read_sensor_data('Indoor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_units_dict(df):\n",
    "#     units_dict = {}\n",
    "#     for column in df.columns:\n",
    "#         units_dict[column] = df.loc[0, column]\n",
    "#     return units_dict\n",
    "\n",
    "# units_dict = create_units_dict(sensor_dataframes[0])\n",
    "\n",
    "units_dict = {'Timestamp': 'UTC',\n",
    " 'Timestamp (Local)': 'UTC-05:00',\n",
    " 'PM 1.0': 'ug/m3',\n",
    " 'PM 2.5': 'ug/m3',\n",
    " 'Applied PM 2.5 Custom Calibration Setting - Multiplication Factor': None,\n",
    " 'Applied PM 2.5 Custom Calibration Setting - Offset': None,\n",
    " 'PM 4.0': 'ug/m3',\n",
    " 'PM 10': 'ug/m3',\n",
    " 'Applied PM 10 Custom Calibration Setting - Multiplication Factor': None,\n",
    " 'Applied PM 10 Custom Calibration Setting - Offset': None,\n",
    " 'NC 0.5': '#/cm3',\n",
    " 'NC 1.0': '#/cm3',\n",
    " 'NC 2.5': '#/cm3',\n",
    " 'NC 4.0': '#/cm3',\n",
    " 'NC 10': '#/cm3',\n",
    " 'PM Sensor Error Status': None,\n",
    " 'CO2': 'ppm',\n",
    " 'Applied CO2 Custom Calibration Setting - Multiplication Factor': None,\n",
    " 'Applied CO2 Custom Calibration Setting - Offset': None,\n",
    " 'CO2 Sensor Error Status': None,\n",
    " 'CH2O': 'ppb',\n",
    " 'Applied CH2O Custom Calibration Setting - Multiplication Factor': None,\n",
    " 'Applied CH2O Custom Calibration Setting - Offset': None,\n",
    " 'CH2O Sensor Error Status': None,\n",
    " 'Barometric Pressure': 'inHg',\n",
    " 'Applied Barometric Pressure Custom Calibration Setting - Offset': None,\n",
    " 'Barometric Sensor Error Status': None,\n",
    " 'CO': 'ppm',\n",
    " 'Applied CO Custom Calibration Setting - Multiplication Factor': None,\n",
    " 'Applied CO Custom Calibration Setting - Offset': None,\n",
    " 'CO Sensor Error Status': None,\n",
    " 'SO2': 'ppb',\n",
    " 'Applied SO2 Custom Calibration Setting - Multiplication Factor': None,\n",
    " 'Applied SO2 Custom Calibration Setting - Offset': None,\n",
    " 'SO2 Sensor Error Status': None,\n",
    " 'O3': 'ppb',\n",
    " 'Applied O3 Custom Calibration Setting - Multiplication Factor': None,\n",
    " 'Applied O3 Custom Calibration Setting - Offset': None,\n",
    " 'O3 Sensor Error Status': None,\n",
    " 'NO2': 'ppb',\n",
    " 'Applied NO2 Custom Calibration Setting - Multiplication Factor': None,\n",
    " 'Applied NO2 Custom Calibration Setting - Offset': None,\n",
    " 'NO2 Sensor Error Status': None,\n",
    " 'VOC tVOC measurement': 'mg/m3',\n",
    " 'Applied TVOC Custom Calibration Setting - Multiplication Factor': None,\n",
    " 'Applied TVOC Custom Calibration Setting - Offset': None,\n",
    " 'VOC Ethanol': 'ppb',\n",
    " 'VOC Sensor Error Status': None,\n",
    " 'Temperature': 'Fahrenheit',\n",
    " 'Applied Temperature Custom Calibration Setting - Offset': 'Celsius',\n",
    " 'Relative Humidity': '%',\n",
    " 'Applied Relative Humidity Custom Calibration Setting - Offset': None,\n",
    " 'Temperature/Humidity Sensor Error Status': None,\n",
    " 'Device Status': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using down\n",
    "\n",
    "# # Sensor names for mapping\n",
    "# sensor_names = ['sensor01', 'sensor02', 'sensor03', 'sensor04', 'sensor05', \n",
    "#                 'sensor06', 'sensor07', 'sensor08', 'sensor09', 'sensor10',\n",
    "#                 'sensor11', 'sensor12', 'sensor13', 'sensor14', 'sensor15']\n",
    "\n",
    "# def clean_sensor_dataframes(sensor_dataframes):\n",
    "#     numeric_columns = ['PM 1.0', 'PM 2.5', 'PM 4.0', 'PM 10',\n",
    "#                        'NC 0.5', 'NC 1.0', 'NC 2.5', 'NC 10',\n",
    "#                        'CO2', 'Barometric Pressure', 'VOC tVOC measurement',\n",
    "#                        'VOC Ethanol', 'Temperature', 'Relative Humidity']\n",
    "#     cleaned_dataframes = []\n",
    "#     for sensor_df, sensor_name in zip(sensor_dataframes, sensor_names):\n",
    "#         # Assign sensor name to each dataframe\n",
    "#         sensor_df['Sensor'] = sensor_name\n",
    "#         # Remove header (if applicable)\n",
    "#         sensor_df = sensor_df.iloc[1:]\n",
    "#         # Sort by 'Timestamp'\n",
    "#         sensor_df = sensor_df.sort_values(by=['Timestamp']).reset_index(drop=True)\n",
    "#         # Change 'Timestamp' to date format\n",
    "#         sensor_df['Timestamp'] = pd.to_datetime(sensor_df['Timestamp'])\n",
    "#         # Convert specified columns to numeric\n",
    "#         sensor_df[numeric_columns] = sensor_df[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "#         # Append the cleaned DataFrame to the list\n",
    "#         cleaned_dataframes.append(sensor_df)\n",
    "#     return cleaned_dataframes\n",
    "\n",
    "# cleaned_sensor_dataframes = clean_sensor_dataframes(sensor_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns to plot\n",
    "working_columns = ['PM 2.5', 'CO2', 'Barometric Pressure', 'VOC tVOC measurement', 'Temperature', 'Relative Humidity']\n",
    "\n",
    "num_plots_per_row = 2\n",
    "for sensor_num, df in enumerate(cleaned_sensor_dataframes):\n",
    "    num_columns = len(working_columns)\n",
    "    num_rows = (num_columns + num_plots_per_row - 1) // num_plots_per_row  # Ceiling division\n",
    "    fig, axs = plt.subplots(num_rows, num_plots_per_row, figsize=(15, 5*num_rows))\n",
    "    fig.suptitle(f\"Sensor {sensor_num + 1}\", fontsize=16)\n",
    "\n",
    "    for i, column in enumerate(working_columns):\n",
    "        row = i // num_plots_per_row\n",
    "        col = i % num_plots_per_row\n",
    "        axs[row, col].plot(df['Timestamp'], df[column])\n",
    "        axs[row, col].set_title(column)\n",
    "        axs[row, col].set_xlabel('Time')\n",
    "        axs[row, col].set_ylabel(f'{column} ({units_dict[column]})')\n",
    "        axs[row, col].grid(True)\n",
    "        axs[row, col].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Harmonization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def read_sensor_data(csv_list):\n",
    "    # csv_list = glob.glob('Lauren purpleair data\\\\*\\\\*.csv')\n",
    "\n",
    "    csv_list\n",
    "    sensor_dataframes = []\n",
    "\n",
    "    for idx,csv_path in enumerate(csv_list):\n",
    "            \n",
    "        sensor_df = pd.read_csv(csv_path)\n",
    "        \n",
    "        if len(sensor_df.columns) < 4:\n",
    "            sensor_df = pd.read_csv(csv_path,delimiter=';')\n",
    "        sensor_dataframes.append(sensor_df)\n",
    "    return sensor_dataframes\n",
    "csv_list = glob.glob('Harmonization\\\\Lauren purpleair data\\\\*\\\\*.csv')\n",
    "sensor_dataframes = read_sensor_data(csv_list) #[:10]\n",
    "sensor_names = [csv_path.split('\\\\')[1] for csv_path in csv_list] #[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sensor_dataframes(sensor_dataframes):\n",
    "    numeric_columns = ['pm2.5_atm_b', 'pm2.5_cf_1', 'pm2.5_cf_1_a', 'pm2.5_cf_1_b',\t'pm10.0_atm', 'pm10.0_atm_a', 'pm10.0_atm_b', 'pm10.0_cf_1', 'pm10.0_cf_1_a', 'pm10.0_cf_1_b']\n",
    "    cleaned_dataframes = []\n",
    "    for sensor_df, sensor_name in zip(sensor_dataframes, sensor_names):\n",
    "        # Assign sensor name to each dataframe\n",
    "        sensor_df['Sensor'] = sensor_name\n",
    "        # Remove header (if applicable)\n",
    "        # sensor_df = sensor_df.iloc[1:]\n",
    "        # Sort by 'Timestamp'\n",
    "        sensor_df = sensor_df.sort_values(by=['time_stamp']).reset_index(drop=True)\n",
    "        # Change 'Timestamp' to date format\n",
    "        sensor_df['time_stamp'] = pd.to_datetime(sensor_df['time_stamp'])\n",
    "        # Convert specified columns to numeric\n",
    "        sensor_df[numeric_columns] = sensor_df[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "        # Append the cleaned DataFrame to the list\n",
    "        cleaned_dataframes.append(sensor_df)\n",
    "    return cleaned_dataframes\n",
    "\n",
    "cleaned_sensor_dataframes = clean_sensor_dataframes(sensor_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_sensor_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_dir = \"Harmonization//parameters\"\n",
    "\n",
    "updated_dataframes = []\n",
    "\n",
    "working_columns = ['pm2.5_atm_b', 'pm2.5_cf_1', 'pm2.5_cf_1_a', 'pm2.5_cf_1_b',\t'pm10.0_atm', 'pm10.0_atm_a', 'pm10.0_atm_b', 'pm10.0_cf_1', 'pm10.0_cf_1_a', 'pm10.0_cf_1_b']\n",
    "\n",
    "for sensor_df in cleaned_sensor_dataframes:\n",
    "    for column_name in working_columns:\n",
    "        # Prepare the path to the coefficients file\n",
    "        file_name = f\"{column_name.lower().replace(' ', '_')}_coefficients_df.csv\"\n",
    "        coefficients_file = os.path.join(parameters_dir, file_name)\n",
    "        \n",
    "        # Read the coefficients data\n",
    "        if os.path.exists(coefficients_file):\n",
    "            coefficients_df = pd.read_csv(coefficients_file)\n",
    "            \n",
    "            # Rename the columns dynamically to prevent overwriting\n",
    "            coefficients_df.rename(columns={'Coefficient': f'{column_name}_Coefficient',\n",
    "                                            'Intercept': f'{column_name}_Intercept'}, inplace=True)\n",
    "            \n",
    "            # Merge the current sensor DataFrame with the coefficients DataFrame\n",
    "            merged_df = pd.merge(sensor_df, coefficients_df, on='Sensor', how='left')\n",
    "            \n",
    "            # Apply the calibration using the dynamically named coefficients and intercepts\n",
    "            merged_df[column_name] = merged_df[column_name] * merged_df[f'{column_name}_Coefficient'] + merged_df[f'{column_name}_Intercept']\n",
    "            \n",
    "            # Drop the dynamically named coefficient and intercept columns after use to clean up\n",
    "            merged_df.drop(columns=[f'{column_name}_Coefficient', f'{column_name}_Intercept'], inplace=True)\n",
    "            \n",
    "            # Update sensor_df for next iteration\n",
    "            sensor_df = merged_df\n",
    "        else:\n",
    "            print(f\"Warning: No coefficients file found for {file_name}\")\n",
    "    \n",
    "    # Append the fully updated DataFrame to the new list\n",
    "    updated_dataframes.append(sensor_df)\n",
    "\n",
    "# Replace the original dataframes list with the updated one\n",
    "cleaned_sensor_dataframes = updated_dataframes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(sensor_df['time_stamp']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pm2.5_atm_b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns to plot\n",
    "# columns_to_plot = ['PM 2.5', 'CO2', 'Barometric Pressure', 'VOC tVOC measurement', 'Temperature', 'Relative Humidity']\n",
    "# working_columns = ['pm2.5_atm_b', 'pm2.5_cf_1', 'pm2.5_cf_1_a', 'pm2.5_cf_1_b',\t'pm10.0_atm', 'pm10.0_atm_a', 'pm10.0_atm_b', 'pm10.0_cf_1', 'pm10.0_cf_1_a', 'pm10.0_cf_1_b']\n",
    "working_columns = ['pm2.5_atm_b', 'pm2.5_cf_1' , 'pm2.5_cf_1_a'] #, 'pm2.5_cf_1_b',\t'pm10.0_atm', 'pm10.0_atm_a', 'pm10.0_atm_b', 'pm10.0_cf_1', 'pm10.0_cf_1_a', 'pm10.0_cf_1_b']\n",
    "\n",
    "\n",
    "num_plots_per_row = 2\n",
    "for sensor_num, df in enumerate(cleaned_sensor_dataframes[:1]):\n",
    "    num_columns = len(working_columns)\n",
    "    num_rows = (num_columns + num_plots_per_row - 1) // num_plots_per_row  # Ceiling division\n",
    "    # fig, axs = plt.subplots(num_rows, num_plots_per_row, figsize=(15, 5*num_rows))\n",
    "    fig, axs = plt.subplots(num_rows, num_plots_per_row, figsize=(10, 3*num_rows))\n",
    "\n",
    "    fig.suptitle(f\"Sensor {sensor_num + 1}\", fontsize=16)\n",
    "    df['time_stamp'] = pd.to_datetime(df['time_stamp'])\n",
    "    \n",
    "    print(df['time_stamp'])\n",
    "\n",
    "    \n",
    "    for i, column in enumerate(working_columns):\n",
    "        row = i // num_plots_per_row\n",
    "        col = i % num_plots_per_row\n",
    "        # axs[row, col].plot(df['time_stamp'], df[column])\n",
    "        print(df[column])\n",
    "        axs[row, col].plot(df['time_stamp'], df[column])\n",
    "        axs[row, col].set_title(column)\n",
    "        axs[row, col].set_xlabel('Time')\n",
    "        axs[row, col].set_ylabel(f'{column}')\n",
    "        axs[row, col].grid(True)\n",
    "        axs[row, col].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(f\"Output/Weekly QA-QC/Jun1-Jun7/sensor_{sensor_num + 1}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_sensor_dataframes[0]['time_stamp'] = pd.to_datetime(cleaned_sensor_dataframes[0]['time_stamp'])\n",
    "cleaned_sensor_dataframes[0]['time_stamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure the 'timestamp' column is in datetime format\n",
    "cleaned_sensor_dataframes[0]['time_stamp'] = pd.to_datetime(cleaned_sensor_dataframes[0]['time_stamp'])\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(cleaned_sensor_dataframes[0]['time_stamp'], cleaned_sensor_dataframes[0]['pa_latency'], marker='o', linestyle='-')\n",
    "plt.xlabel('time_stamp')\n",
    "plt.ylabel('PA Latency')\n",
    "plt.title('Timestamp vs PA Latency')\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
